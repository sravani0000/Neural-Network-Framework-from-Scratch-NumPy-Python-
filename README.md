Project Overview: Built a fully functional multi-layer perceptron (MLP) from the ground up using only NumPy. This project demonstrates a deep mathematical understanding of backpropagation and gradient descent without relying on high-level frameworks like Keras or PyTorch.

Technical Highlights:

Mathematical Implementation: Manually coded the forward pass, cost functions (Mean Squared Error), and backward pass (gradient computation).

Optimization: Implemented the Delta Learning Rule and compared performance between different learning rates (0.5 vs. 0.01) to identify optimal convergence paths.

Generalization: Tested the framework on non-linear datasets to validate the network's ability to learn complex decision boundaries.
